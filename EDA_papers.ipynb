{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EDA_papers.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "sSEJtikTHsCR",
        "x1UQYfUqH6Ek"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JV2NiCeLaIn"
      },
      "source": [
        "Lectura de datos.cvs\n",
        "\n",
        "Analisis exploratorio de datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpQBbzFCw08R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30c91bd0-3172-47e4-92ac-6d9b022171ca"
      },
      "source": [
        "!pip install -qq contractions\n",
        "!pip install -qq autocorrect\n",
        "!pip install multidict\n",
        "!pip install -qq ipython-autotime\n",
        "!pip install -qq -U scikit-learn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: multidict in /usr/local/lib/python3.7/dist-packages (5.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBGDJyZnPGyG"
      },
      "source": [
        "import io\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.model_selection as model_selection\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from spacy.attrs import POS\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.semi_supervised import SelfTrainingClassifier\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V04B4GWAiZm"
      },
      "source": [
        "---\n",
        "# **<font color='blue'>Carga de archivo cvs</font>**\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhNka7OcqzsZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc2e2799-f098-4bc5-ec75-c4430b00c90b"
      },
      "source": [
        "%matplotlib inline\n",
        "%load_ext autotime"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 109 µs (started: 2021-12-09 01:08:03 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiUbxDYAoypC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8435c7b3-0105-4050-b936-5868d28f3755"
      },
      "source": [
        "# es el cvs con las busquedas en scopus de 700 papers sobre robotica y simuladores con algunos ya clasificados\n",
        "#url = 'https://raw.githubusercontent.com/gzabala/ml/main/query_5_classified_out.csv'\n",
        "url = 'https://raw.githubusercontent.com/NestorBalich/IA-ML/main/dfscopus.csv'\n",
        "dfaux = pd.read_csv(url, encoding='latin-1', sep=',',index_col=False)\n",
        "print(\"Cantidad de columnas: \",len(dfaux.columns))\n",
        "print(\"Cantidad de filas: \",len(dfaux))\n",
        "\n",
        "# dfaux.to_csv(\"dtscopus.csv\",index=False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cantidad de columnas:  16\n",
            "Cantidad de filas:  700\n",
            "time: 140 ms (started: 2021-12-09 01:08:03 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UW8DbsHUlLa"
      },
      "source": [
        "---\n",
        "# **<font color='blue'>Primer selección de features</font>**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dkOb0dq5dFp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4960d420-a97e-47b7-f696-40dff65c7cd3"
      },
      "source": [
        "from google.colab import data_table\n",
        "\n",
        "%reload_ext google.colab.data_table\n",
        "data_table.enable_dataframe_formatter()\n",
        "data_table.DataTable(dfaux, include_index=False, num_rows_per_page=10)\n",
        "dfaux.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/a6224c040fa35dcf/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n\"10.5755/j01.eee.18.9.2825\",\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n\"The LEGO NXT robot-based e-learning environment to teach computer science topics\",\n\"ar\",\n\"Article\",\n\"Burbaite R.\",\n\"Kaunas University of Technology\",\n\"Kaunas\",\n\"Lithuania\",\n\"Elektronika ir Elektrotechnika\",\n\"Journal\",\n\"18\",\n\"113-116\",\n{\n            'v': 12,\n            'f': \"12\",\n        },\n\"It is difficult to motivate learners to learn abstract Computer Science topics (e.g., data structures, algorithms and programming) with the adequate level of engagement. We present the process of constructing a LEGO robot, called the DRAWBOT (drawing robot), which enables to create the e-learning environment to demonstrate visually the solution of graph-based Computer Science tasks through teaching programming. Our research has confirmed the importance of using robot-based environments for teaching that was known so far in the literature on e-learning. We have extended the known approaches: a) by providing technical characteristics for the process to create the e-learning environment for the real setting; b) by smoothly integrating different phases of the process and considering it into entirety to support the constructivist learning model.\\n\\nDOI: http://dx.doi.org/10.5755/j01.eee.18.9.2825\",\n\"SI\"],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n\"10.1080/03043797.2012.725711\",\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n\"Embedded C programming: A practical course introducing programmable microprocessors\",\n\"ar\",\n\"Article\",\n\"Laverty D.\",\n\"Queen's University Belfast\",\n\"Belfast\",\n\"United Kingdom\",\n\"European Journal of Engineering Education\",\n\"Journal\",\n\"37\",\n\"557-574\",\n{\n            'v': 3,\n            'f': \"3\",\n        },\n\"This paper presents a new laboratory-based module for embedded systems teaching, which addresses the current lack of consideration for the link between hardware development, software implementation, course content and student evaluation in a laboratory environment. The course introduces second year undergraduate students to the interface between hardware and software and the programming of embedded devices; in this case, the PIC (originally peripheral interface controller, later rebranded programmable intelligent computer) microcontroller. A hardware development board designed for use in the laboratories of this module is presented. Through hands on laboratory experience, students are encouraged to engage with practical problem-solving exercises and develop programming skills across a broad range of scenarios.\",\n\"NO\"]],\n        columns: [[\"number\", \"index\"], [\"string\", \"doi\"], [\"number\", \"pubmed_id\"], [\"string\", \"title\"], [\"string\", \"subtype\"], [\"string\", \"subtypeDescription\"], [\"string\", \"creator\"], [\"string\", \"affilname\"], [\"string\", \"affiliation_city\"], [\"string\", \"affiliation_country\"], [\"string\", \"publicationName\"], [\"string\", \"aggregationType\"], [\"string\", \"volume\"], [\"string\", \"pageRange\"], [\"number\", \"citedby_count\"], [\"string\", \"abstract\"], [\"string\", \"include?\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    ",
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doi</th>\n",
              "      <th>pubmed_id</th>\n",
              "      <th>title</th>\n",
              "      <th>subtype</th>\n",
              "      <th>subtypeDescription</th>\n",
              "      <th>creator</th>\n",
              "      <th>affilname</th>\n",
              "      <th>affiliation_city</th>\n",
              "      <th>affiliation_country</th>\n",
              "      <th>publicationName</th>\n",
              "      <th>aggregationType</th>\n",
              "      <th>volume</th>\n",
              "      <th>pageRange</th>\n",
              "      <th>citedby_count</th>\n",
              "      <th>abstract</th>\n",
              "      <th>include?</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10.5755/j01.eee.18.9.2825</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The LEGO NXT robot-based e-learning environmen...</td>\n",
              "      <td>ar</td>\n",
              "      <td>Article</td>\n",
              "      <td>Burbaite R.</td>\n",
              "      <td>Kaunas University of Technology</td>\n",
              "      <td>Kaunas</td>\n",
              "      <td>Lithuania</td>\n",
              "      <td>Elektronika ir Elektrotechnika</td>\n",
              "      <td>Journal</td>\n",
              "      <td>18</td>\n",
              "      <td>113-116</td>\n",
              "      <td>12</td>\n",
              "      <td>It is difficult to motivate learners to learn ...</td>\n",
              "      <td>SI</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10.1080/03043797.2012.725711</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Embedded C programming: A practical course int...</td>\n",
              "      <td>ar</td>\n",
              "      <td>Article</td>\n",
              "      <td>Laverty D.</td>\n",
              "      <td>Queen's University Belfast</td>\n",
              "      <td>Belfast</td>\n",
              "      <td>United Kingdom</td>\n",
              "      <td>European Journal of Engineering Education</td>\n",
              "      <td>Journal</td>\n",
              "      <td>37</td>\n",
              "      <td>557-574</td>\n",
              "      <td>3</td>\n",
              "      <td>This paper presents a new laboratory-based mod...</td>\n",
              "      <td>NO</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                            doi  ...  include?\n",
              "0     10.5755/j01.eee.18.9.2825  ...        SI\n",
              "1  10.1080/03043797.2012.725711  ...        NO\n",
              "\n",
              "[2 rows x 16 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 56.1 ms (started: 2021-12-09 01:08:03 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dT-KmtoUZy6E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6caada9-4933-4f47-db3b-b77a163e679a"
      },
      "source": [
        "# Estos son los features del dataset\n",
        "dfaux.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['doi', 'pubmed_id', 'title', 'subtype', 'subtypeDescription', 'creator',\n",
              "       'affilname', 'affiliation_city', 'affiliation_country',\n",
              "       'publicationName', 'aggregationType', 'volume', 'pageRange',\n",
              "       'citedby_count', 'abstract', 'include?'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 7.6 ms (started: 2021-12-09 01:08:03 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UGganjEBFpN"
      },
      "source": [
        "---\n",
        "## **<font color='blue'>Buscamos y eliminamos columnas sin datos</font>**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCy_T838U6Ki"
      },
      "source": [
        "---\n",
        "## **<font color='blue'>Evaluamos y eliminamos columnas que no usaremos</font>**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Describiremos cada feature y decidiremos si la conservamos o no**\n",
        "\n",
        "doi: el doi es un identificador de un documento electrónico. No aporta información relacionada con nuestro target.\n",
        "\n",
        "pubmed_id: es un identificador especial de las publicaciones médicas. Puede ser relevante dado que define una categoría de pertenencia de la publicación que puede tener vínculo con nuestro target.\n",
        "\n",
        "title: título de la publicación. Lo tokenizaremos para obtener los stems correspondientes. Entendemos que tiene una fuerte correlación con la pertinencia o no del paper al campo de búsqueda.\n",
        "\n",
        "subtype: abreviatura de subtype description.\n",
        "\n",
        "subtypeDescription: indica si es un artículo, review, conference, errata u otro. Vamos a conservar subtype dado que representan lo mismo.\n",
        "   \n",
        "creator: primer autor del artículo. Es altamente probable que un mismo autor escriba artículos de un mismo campo de estudio, y por lo tanto, esté correlacionado con la pertinencia.\n",
        "\n",
        "affilname: institución del primer autor. También es probable que en una misma institución se desarrollo en varios artículos sobre el mismo tema.\n",
        "\n",
        "affiliation_city: ciudad de la institución. No consideramos que aporte para la clasificación.\n",
        "\n",
        "affiliation_country: país de la institución. Idem anterior.\n",
        "\n",
        "publicationName: nombre de la publicación del artículo. Entendemos que hay una correlación entre la publicación y la pertinencia o no de sus artículos para nuestra investigación.\n",
        "\n",
        "aggregationType: journal, Book, etc. También consideramos que puede tener alguna correlación de la misma forma que subtype.\n",
        "\n",
        "volume: número de volumen en la colección. Sin correlación. \n",
        "\n",
        "pageRange: si está publicado en un libro o revista, cuáles son los nros de páginas dentro de la publicación. No aporta a la pertinencia.\n",
        "\n",
        "citedby_count: cantidad de citas. A pesar de que puede ser interesante para determinar calidad del artículo, no se relaciona con nuestro feature destino.\n",
        " \n",
        "\n",
        "abstract: del artículo. Junto con el título, consideramos que son los features que más aportarán a la creación del modelo.\n",
        "\n",
        "include?: pertinencia o no del artículo a mi mapeo. Target feature."
      ],
      "metadata": {
        "id": "Pjf4mafWuWHR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmtxmRSuauXY",
        "outputId": "5d5f83cd-1546-490b-9b78-59a1ec9a7a71"
      },
      "source": [
        "#vemos si hay columnas que contengan todos null\n",
        "df1 = dfaux.copy()\n",
        "df1.isna().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "doi                      0\n",
              "pubmed_id              671\n",
              "title                    0\n",
              "subtype                  0\n",
              "subtypeDescription       0\n",
              "creator                  0\n",
              "affilname                5\n",
              "affiliation_city         7\n",
              "affiliation_country      5\n",
              "publicationName          0\n",
              "aggregationType          0\n",
              "volume                  32\n",
              "pageRange              168\n",
              "citedby_count            0\n",
              "abstract                35\n",
              "include?                 0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 14.7 ms (started: 2021-12-09 01:08:03 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUGbJrc5UzIp",
        "outputId": "09b2c35a-997e-4de6-9821-92b75f83a9a0"
      },
      "source": [
        "df1.drop('doi', axis='columns', inplace=True)\n",
        "df1.drop('subtypeDescription', axis='columns', inplace=True)\n",
        "df1.drop('affiliation_city', axis='columns', inplace=True)   \n",
        "df1.drop('affiliation_country', axis='columns', inplace=True)   \n",
        "df1.drop('volume', axis='columns', inplace=True)   \n",
        "df1.drop('pageRange', axis='columns', inplace=True)   \n",
        "df1.drop('citedby_count', axis='columns', inplace=True)\n",
        "df1.isna().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pubmed_id          671\n",
              "title                0\n",
              "subtype              0\n",
              "creator              0\n",
              "affilname            5\n",
              "publicationName      0\n",
              "aggregationType      0\n",
              "abstract            35\n",
              "include?             0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 25.3 ms (started: 2021-12-09 01:08:03 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElellIU7bnnN",
        "outputId": "0b2fbab6-ea36-4004-a661-91dcdfe0505d"
      },
      "source": [
        "# que tipo de datos tiene cada una\n",
        "df1.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 700 entries, 0 to 699\n",
            "Data columns (total 9 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   pubmed_id        29 non-null     float64\n",
            " 1   title            700 non-null    object \n",
            " 2   subtype          700 non-null    object \n",
            " 3   creator          700 non-null    object \n",
            " 4   affilname        695 non-null    object \n",
            " 5   publicationName  700 non-null    object \n",
            " 6   aggregationType  700 non-null    object \n",
            " 7   abstract         665 non-null    object \n",
            " 8   include?         700 non-null    object \n",
            "dtypes: float64(1), object(8)\n",
            "memory usage: 49.3+ KB\n",
            "time: 14 ms (started: 2021-12-09 01:08:03 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBuLGZLLuZf8"
      },
      "source": [
        "---\n",
        "# **<font color='blue'>Normalización</font>**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN5AJhgruN6A"
      },
      "source": [
        "---\n",
        "## **<font color='blue'>Funciones de normalización</font>**\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWLHnW-b79rv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74c30371-19e7-4ddd-83fa-19eb38c66f68"
      },
      "source": [
        "#Declaracion de clase con comportamiento global\n",
        "\n",
        "import re             # nos permite trabajar con expresiones regulares\n",
        "import unicodedata    # para poder eliminar acentos y nomalizar a unicode\n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize.toktok import ToktokTokenizer # utilizada para las stopword\n",
        "from nltk.tokenize import  word_tokenize # divide en tokens\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk_stopwords = nltk.corpus.stopwords.words('english')\n",
        "nltk.download('punkt') # cargamos tokenizador\n",
        "\n",
        "from bs4 import BeautifulSoup #agregamos para eliminar tags\n",
        "\n",
        "nltk.download('wordnet')  #wordnet es una base de datos léxica para mas de 200 idiomas \n",
        "from nltk.stem import WordNetLemmatizer \n",
        "\n",
        "#lematizacion es cambiar la palabra por la de entrada en un diccionario\n",
        "#ref: https://es.wikipedia.org/wiki/Lematizaci%C3%B3n#:~:text=La%20lematizaci%C3%B3n%20es%20un%20proceso,flexionadas%20de%20una%20misma%20palabra.\n",
        "import spacy          # usamos de forma general para lematizar\n",
        "nlp = spacy.load(\"en_core_web_sm\") # carga el core para soporte de idiomas https://spacy.io/usage/models\n",
        "                                   # se saca afuera de la función para mejorar el tiempo de lematización\n",
        "\n",
        "import contractions\n",
        "import re\n",
        "# Utilizamos autocorrect porque obtuvimos una mejor performance que spellchecker en nuestros tests\n",
        "from autocorrect import Speller\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "class Normalization:\n",
        "  stemmer = SnowballStemmer(language='english')\n",
        "  check = Speller(lang='en',fast=True)\n",
        "\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  @staticmethod\n",
        "  def lemmatize_text_spacy(text):\n",
        "    doc = nlp(text)\n",
        "    lemmatized = ''\n",
        "    for word in doc:\n",
        "      if not word.lemma_.startswith('-') :\n",
        "        lemmatized += word.lemma_ + ' '\n",
        "        \n",
        "    return lemmatized\n",
        "\n",
        "  @staticmethod\n",
        "  def stemmer_text_spacy(text):\n",
        "    doc = nlp(text)\n",
        "    stemmized=''\n",
        "    for word in doc:\n",
        "      stemmized += Normalization.stemmer.stem(word.text)+' '\n",
        "    return stemmized.strip()\n",
        "\n",
        "  @staticmethod\n",
        "  def remove_stopwords(text,is_lower_case=False):\n",
        "      tokens = ToktokTokenizer().tokenize(text)\n",
        "\n",
        "      tokens = [token.strip() for token in tokens]\n",
        "      if is_lower_case:\n",
        "          filtered_tokens = [token for token in tokens if token not in nltk_stopwords]\n",
        "      else:\n",
        "          filtered_tokens = [token for token in tokens if token.lower() not in nltk_stopwords]\n",
        "      filtered_text = ' '.join(filtered_tokens)\n",
        "      return filtered_text\n",
        "\n",
        "  @staticmethod\n",
        "  def remove_special_characters(text, remove_digits=False):\n",
        "      pattern = r'[՜^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
        "      text = re.sub(pattern, '', text)\n",
        "      return text\n",
        "  #para ver un caracte ren especial: chr(dic_frec_proc[''])\n",
        "\n",
        "# Utilizamos autocorrect porque obtuvimos una mejor performance que spellchecker en nuestros tests\n",
        "  @staticmethod\n",
        "  def spell_check(text):\n",
        "    return Normalization.check(text)\n",
        "\n",
        "  @staticmethod\n",
        "  def strip_html_tags(text):\n",
        "      soup = BeautifulSoup(text, \"html.parser\")\n",
        "      [s.extract() for s in soup(['iframe', 'script'])] # eliminamos los elementos de tipo iframe y script, si quisieramos recuperarlos y no solo borrarlos, deberíamos utilizar decompose\n",
        "      stripped_text = soup.get_text()\n",
        "      stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text) # reemplazamos secuencias de saltos de línea por uno solo\n",
        "      return stripped_text\n",
        "\n",
        "  @staticmethod\n",
        "  def delete_black_word(texto):\n",
        "    black_list = [\"՜\",\"\"]\n",
        "    splitText = texto.split()\n",
        "    out = ' '.join([i for i in splitText if i not in black_list])\n",
        "    return out\n",
        "\n",
        "  @staticmethod\n",
        "  def delete_not_white_char(texto):\n",
        "    import re\n",
        "    result = re.sub('[^a-zA-Z0-9]', '', texto)\n",
        "    return out\n",
        "\n",
        "      #remueve contenido especifico urls hashtag\n",
        "  @staticmethod\n",
        "  def remove_content(text):\n",
        "      text = re.sub(r\"http\\S+\", \"\", text) #remove urls\n",
        "      text=re.sub(r'\\S+\\.com\\S+','',text) #remove urls\n",
        "      text=re.sub(r'\\@\\w+','',text) #remove mentions\n",
        "      text =re.sub(r'\\#\\w+','',text) #remove hashtags\n",
        "      return text\n",
        "   \n",
        "  @staticmethod\n",
        "  def get_only_char_to_lower(line):\n",
        "        clean_line = \"\"\n",
        "\n",
        "        line = line.replace(\"’\", \"\")\n",
        "        line = line.replace(\"'\", \"\")\n",
        "        line = line.replace(\"-\", \"\") #replace hyphens with spaces\n",
        "        line = line.replace(\"\\t\", \"\")\n",
        "        line = line.replace(\"\\n\", \"\")\n",
        "        line = line.lower()\n",
        "\n",
        "        for char in line:\n",
        "            if char in 'qwertyuiopasdfghjklzxcvbnm ':\n",
        "                clean_line += char\n",
        "            else:\n",
        "                clean_line += ' '\n",
        "\n",
        "        # clean_line = re.sub(' +',' ',clean_line) #delete extra spaces\n",
        "        # if clean_line[0] == ' ':\n",
        "        #     clean_line = clean_line[1:]\n",
        "        return clean_line\n",
        "\n",
        "def normalizar_text(dataf,\n",
        "                    n_feature,\n",
        "                    n_eliminar_rows_con_feature_nulo,\n",
        "                    n_reemplazar_feature_nulo_por_unknow ,\n",
        "                    n_eliminar_espacios_iniciales_finales ,\n",
        "                    n_pasar_minuscula ,\n",
        "                    n_eliminar_espacios ,\n",
        "                    n_eliminar_stopwords,\n",
        "                    n_expandir_contracciones,\n",
        "                    n_eliminar_caracteres_especiales,\n",
        "                    n_correccion_ortografica,\n",
        "                    n_reemplar_doble_espacios,\n",
        "                    n_eliminar_acentos,\n",
        "                    n_elimina_tag,\n",
        "                    n_elimina_menciones,\n",
        "                    n_elimina_to_black_word_list,\n",
        "                    n_elimina_sino_esta_en_white_char,\n",
        "                    n_lemmatize_text_spacy,\n",
        "                    n_stemmer_text_spacy,\n",
        "                    n_get_only_char_to_lower,\n",
        "                    n_view):\n",
        "\n",
        "    n_proc_feature = \"proc_\" + n_feature \n",
        "    ldataNorm = dataf.copy()\n",
        "    ldataNorm.loc[:,n_proc_feature]=dataf.loc[:,n_feature]\n",
        "\n",
        "    if (n_view ):\n",
        "        print(\"\\n=======================================\") \n",
        "        print(\"* Normalización de feature text: \",n_feature , \" en \", n_proc_feature) \n",
        "\n",
        "    if n_eliminar_rows_con_feature_nulo:  \n",
        "        nulos = ldataNorm[n_feature].isna().sum()\n",
        "        if (n_view ):print(\"* Remover los rows con nulos, se removieron:\", nulos)\n",
        "        ldataNorm.drop(ldataNorm[ldataNorm[n_feature].isna()].index, inplace=True)\n",
        "\n",
        "    if n_reemplazar_feature_nulo_por_unknow:  \n",
        "      # Rellenar affilname nan con Unknown\n",
        "      if (n_view ):print(\"* Reemplazar las columnas con nulos por unknown\")\n",
        "      ldataNorm[[n_proc_feature]] = ldataNorm[[n_proc_feature]].fillna(value='unknown')\n",
        "    if n_eliminar_espacios_iniciales_finales:  \n",
        "        if (n_view ):print(\"* Eliminamos los espacios iniciales y finales\") \n",
        "        ldataNorm.loc[:,n_proc_feature] = ldataNorm.loc[:,n_proc_feature].str.strip()\n",
        "    if n_pasar_minuscula:  \n",
        "        if (n_view ):print(\"* Pasa palabras a minúsculas\")\n",
        "        ldataNorm.loc[:,n_proc_feature]  = ldataNorm.loc[:,n_proc_feature].apply(lambda x : x.lower() )\n",
        "    if n_eliminar_espacios:  \n",
        "        if (n_view ):print(\"* Removemos espacios en blanco extras\")\n",
        "        ldataNorm.loc[:,n_proc_feature]  = ldataNorm.loc[:,n_proc_feature].apply(lambda x:  re.sub(' +', ' ', x))\n",
        "    if n_eliminar_stopwords: \n",
        "        if (n_view ):print(\"* Removemos las stopwords\")\n",
        "        ldataNorm.loc[:,n_proc_feature]  = ldataNorm.loc[:,n_proc_feature].apply(lambda x: Normalization.remove_stopwords(x, True)) \n",
        "    if n_expandir_contracciones: \n",
        "        if (n_view ):print(\"* Expandimos las contracciones en ingles\")\n",
        "        ldataNorm.loc[:,n_proc_feature]  = ldataNorm.loc[:,n_proc_feature].apply(lambda x: contractions.fix(x))\n",
        "    if n_eliminar_caracteres_especiales:     \n",
        "        if (n_view ):print(\"* Removemos los carácteres especiales\")\n",
        "        ldataNorm.loc[:,n_proc_feature]  = ldataNorm.loc[:,n_proc_feature].apply(lambda x: Normalization.remove_special_characters(x, True))\n",
        "    if n_correccion_ortografica:    \n",
        "        if (n_view ):print(\"* Realizamos corrección ortográfica rapida en ingles\")\n",
        "        ldataNorm.loc[:,n_proc_feature]  = ldataNorm.loc[:,n_proc_feature].apply(lambda x : Normalization.spell_check(x) )\n",
        "    if n_reemplar_doble_espacios:    \n",
        "        if (n_view ):print(\"* Reemplazamos los doble espacios por uno solo\")\n",
        "        ldataNorm.loc[:,n_proc_feature]  = ldataNorm.loc[:,n_proc_feature].str.replace('  ', ' ')\n",
        "    if n_lemmatize_text_spacy:    \n",
        "        if (n_view ):print(\"* Lematizamos\")\n",
        "        ldataNorm.loc[:,n_proc_feature] = ldataNorm.loc[:,n_proc_feature].apply(lambda x: Normalization.lemmatize_text_spacy(str(x)))\n",
        "    if n_stemmer_text_spacy:    \n",
        "        if (n_view ):print(\"* Stemizamos\")\n",
        "        ldataNorm.loc[:,n_proc_feature] = ldataNorm.loc[:,n_proc_feature].apply(lambda x: Normalization.stemmer_text_spacy(str(x)))\n",
        "\n",
        "    #remueve los acentos  //Ojo esta funcion pone en mayuscula la primer letra por eso la conversión a minúscula se hace a continuación en el último paso por seguridad\n",
        "    if n_eliminar_acentos: \n",
        "      if (n_view ):print(\"* Removemos acentos\")\n",
        "      ldataNorm.loc[n_proc_feature] = ldataNorm.loc[:,n_proc_feature].apply(lambda x: Normalization.remove_accented_chars(x))\n",
        "    if n_elimina_tag: \n",
        "      if (n_view ):print(\"* Removemos los tags\")\n",
        "      ldataNorm.loc[:,n_proc_feature] = ldataNorm.loc[:,n_proc_feature].apply(lambda x:  Normalization.strip_html_tags(x))\n",
        "    if n_elimina_menciones: \n",
        "      if (n_view ):print(\"* Removemos las menciones\")\n",
        "      ldataNorm.loc[:,n_proc_feature] = ldataNorm.loc[:,n_proc_feature].apply(lambda x: Normalization.remove_content(x))\n",
        "    if n_elimina_to_black_word_list: \n",
        "      if (n_view ):print(\"* Removemos las palabras que estan en lista negra\")\n",
        "      ldataNorm.loc[:,n_proc_feature] = ldataNorm.loc[:,n_proc_feature].apply(lambda x: Normalization.delete_black_word(x)) \n",
        "    if n_elimina_sino_esta_en_white_char: \n",
        "      if (n_view ):print(\"* Removemos los caracteres que no esten en lista blanca\")\n",
        "      ldataNorm.loc[:,n_proc_feature]  = ldataNorm.loc[:,n_proc_feature].apply(lambda x: re.sub('[^a-zA-Z0-9]+$', '', x))\n",
        "\n",
        "    if n_get_only_char_to_lower:    \n",
        "        if (n_view ):print(\"* Deja solo letras y pasa todo a minuscula\")\n",
        "        ldataNorm.loc[:,n_proc_feature] = ldataNorm.loc[:,n_proc_feature].apply(lambda x: Normalization.get_only_char_to_lower(str(x)))\n",
        "    \n",
        "    if (n_view ): print(\"=======================================\") \n",
        "\n",
        "    return ldataNorm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "time: 1.72 s (started: 2021-12-09 01:08:03 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwuHG7E67vQ8"
      },
      "source": [
        "---\n",
        "## **<font color='blue'>Normalización de cada feature (configurable)</font>**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iY6Gg39yDbcP",
        "outputId": "f092c5ce-275a-4ccf-8e92-8a8bdcc4f7bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['pubmed_id', 'title', 'subtype', 'creator', 'affilname',\n",
              "       'publicationName', 'aggregationType', 'abstract', 'include?'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 6.77 ms (started: 2021-12-09 01:08:05 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgM_QL0evWMc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69aaa886-a1e4-49dc-d9ad-ef7d25fff5dc"
      },
      "source": [
        "#Normalización pubmed_id\n",
        "df1['proc_pubmed_id']=df1['pubmed_id'].apply(lambda x: 1 if x>0 else 0)\n",
        "df1[['pubmed_id','proc_pubmed_id']].head(50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/a6224c040fa35dcf/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 4,\n            'f': \"4\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 5,\n            'f': \"5\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 6,\n            'f': \"6\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 7,\n            'f': \"7\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 8,\n            'f': \"8\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 9,\n            'f': \"9\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 10,\n            'f': \"10\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 11,\n            'f': \"11\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 12,\n            'f': \"12\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 13,\n            'f': \"13\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 14,\n            'f': \"14\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 15,\n            'f': \"15\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 16,\n            'f': \"16\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 17,\n            'f': \"17\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 18,\n            'f': \"18\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 19,\n            'f': \"19\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 20,\n            'f': \"20\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 21,\n            'f': \"21\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 22,\n            'f': \"22\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 23,\n            'f': \"23\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 24,\n            'f': \"24\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 25,\n            'f': \"25\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 26,\n            'f': \"26\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 27,\n            'f': \"27\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 28,\n            'f': \"28\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 29,\n            'f': \"29\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 30,\n            'f': \"30\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 31,\n            'f': \"31\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 32,\n            'f': \"32\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 33,\n            'f': \"33\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 34,\n            'f': \"34\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 35,\n            'f': \"35\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 36,\n            'f': \"36\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 37,\n            'f': \"37\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 38,\n            'f': \"38\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 39,\n            'f': \"39\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 40,\n            'f': \"40\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 41,\n            'f': \"41\",\n        },\n{\n            'v': 23098753.0,\n            'f': \"23098753.0\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        }],\n [{\n            'v': 42,\n            'f': \"42\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 43,\n            'f': \"43\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 44,\n            'f': \"44\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 45,\n            'f': \"45\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 46,\n            'f': \"46\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 47,\n            'f': \"47\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 48,\n            'f': \"48\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 49,\n            'f': \"49\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }]],\n        columns: [[\"number\", \"index\"], [\"number\", \"pubmed_id\"], [\"number\", \"proc_pubmed_id\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    ",
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pubmed_id</th>\n",
              "      <th>proc_pubmed_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>23098753.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     pubmed_id  proc_pubmed_id\n",
              "0          NaN               0\n",
              "1          NaN               0\n",
              "2          NaN               0\n",
              "3          NaN               0\n",
              "4          NaN               0\n",
              "5          NaN               0\n",
              "6          NaN               0\n",
              "7          NaN               0\n",
              "8          NaN               0\n",
              "9          NaN               0\n",
              "10         NaN               0\n",
              "11         NaN               0\n",
              "12         NaN               0\n",
              "13         NaN               0\n",
              "14         NaN               0\n",
              "15         NaN               0\n",
              "16         NaN               0\n",
              "17         NaN               0\n",
              "18         NaN               0\n",
              "19         NaN               0\n",
              "20         NaN               0\n",
              "21         NaN               0\n",
              "22         NaN               0\n",
              "23         NaN               0\n",
              "24         NaN               0\n",
              "25         NaN               0\n",
              "26         NaN               0\n",
              "27         NaN               0\n",
              "28         NaN               0\n",
              "29         NaN               0\n",
              "30         NaN               0\n",
              "31         NaN               0\n",
              "32         NaN               0\n",
              "33         NaN               0\n",
              "34         NaN               0\n",
              "35         NaN               0\n",
              "36         NaN               0\n",
              "37         NaN               0\n",
              "38         NaN               0\n",
              "39         NaN               0\n",
              "40         NaN               0\n",
              "41  23098753.0               1\n",
              "42         NaN               0\n",
              "43         NaN               0\n",
              "44         NaN               0\n",
              "45         NaN               0\n",
              "46         NaN               0\n",
              "47         NaN               0\n",
              "48         NaN               0\n",
              "49         NaN               0"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 30.2 ms (started: 2021-12-09 01:08:05 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iENM9yftrPU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac6e0da7-8008-40f0-f0a9-8ad1ecff7c05"
      },
      "source": [
        "#@title Normalization (texto): abstract\n",
        "n_feature = \"abstract\" \n",
        "n_eliminar_rows_con_feature_nulo = True #@param {type:\"boolean\"}\n",
        "n_reemplazar_feature_nulo_por_unknown = False #@param {type:\"boolean\"}\n",
        "n_eliminar_espacios_iniciales_finales = True #@param {type:\"boolean\"}\n",
        "n_pasar_minuscula = True #@param {type:\"boolean\"}\n",
        "n_eliminar_espacios = True #@param {type:\"boolean\"}\n",
        "n_eliminar_stopwords = True #@param {type:\"boolean\"}\n",
        "n_expandir_contracciones = True #@param {type:\"boolean\"}\n",
        "n_eliminar_caracteres_especiales = True #@param {type:\"boolean\"}\n",
        "n_correccion_ortografica = True #@param {type:\"boolean\"}\n",
        "n_reemplar_doble_espacios = True #@param {type:\"boolean\"}\n",
        "n_eliminar_acentos = False #@param {type:\"boolean\"}\n",
        "n_elimina_tag = False #@param {type:\"boolean\"}\n",
        "n_elimina_menciones = False #@param {type:\"boolean\"}\n",
        "n_elimina_black_word_list = True #@param {type:\"boolean\"}\n",
        "n_elimina_sino_esta_en_white_char_list = False #@param {type:\"boolean\"}\n",
        "n_lemmatize_text_spacy = True #@param {type:\"boolean\"}\n",
        "n_stemmer_text_spacy = False #@param {type:\"boolean\"}\n",
        "n_get_only_char_to_lower = True #@param {type:\"boolean\"}\n",
        "\n",
        "df1 = normalizar_text(df1,n_feature, n_eliminar_rows_con_feature_nulo,\n",
        "                n_reemplazar_feature_nulo_por_unknown ,\n",
        "                n_eliminar_espacios_iniciales_finales ,\n",
        "                n_pasar_minuscula ,\n",
        "                n_eliminar_espacios ,\n",
        "                n_eliminar_stopwords,\n",
        "                n_expandir_contracciones,\n",
        "                n_eliminar_caracteres_especiales,\n",
        "                n_correccion_ortografica,\n",
        "                n_reemplar_doble_espacios,\n",
        "                n_eliminar_acentos,\n",
        "                n_elimina_tag,\n",
        "                n_elimina_menciones,\n",
        "                n_elimina_black_word_list,\n",
        "                n_elimina_sino_esta_en_white_char_list,\n",
        "                n_lemmatize_text_spacy,\n",
        "                n_stemmer_text_spacy,\n",
        "                n_get_only_char_to_lower,\n",
        "                True)\n",
        "\n",
        "df1[['abstract','proc_abstract']].head(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=======================================\n",
            "* Normalización de feature text:  abstract  en  proc_abstract\n",
            "* Remover los rows con nulos, se removieron: 35\n",
            "* Eliminamos los espacios iniciales y finales\n",
            "* Pasa palabras a minúsculas\n",
            "* Removemos espacios en blanco extras\n",
            "* Removemos las stopwords\n",
            "* Expandimos las contracciones en ingles\n",
            "* Removemos los carácteres especiales\n",
            "* Realizamos corrección ortográfica rapida en ingles\n",
            "* Reemplazamos los doble espacios por uno solo\n",
            "* Lematizamos\n",
            "* Removemos las palabras que estan en lista negra\n",
            "* Deja solo letras y pasa todo a minuscula\n",
            "=======================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/a6224c040fa35dcf/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n\"It is difficult to motivate learners to learn abstract Computer Science topics (e.g., data structures, algorithms and programming) with the adequate level of engagement. We present the process of constructing a LEGO robot, called the DRAWBOT (drawing robot), which enables to create the e-learning environment to demonstrate visually the solution of graph-based Computer Science tasks through teaching programming. Our research has confirmed the importance of using robot-based environments for teaching that was known so far in the literature on e-learning. We have extended the known approaches: a) by providing technical characteristics for the process to create the e-learning environment for the real setting; b) by smoothly integrating different phases of the process and considering it into entirety to support the constructivist learning model.\\n\\nDOI: http://dx.doi.org/10.5755/j01.eee.18.9.2825\",\n\"difficult motivate learner learn abstract computer science topic eg data structure algorithm program adequate level engagement present process construct lego robot call drawbot drawing robot enable create learn environment demonstrate visually solution graphbase computer science task teach programming research confirm importance use robotbased environment teach know far literature learn extend know approach provide technical characteristic process create learn environment real setting b smoothly integrate different phase process consider entirety support constructivist learn model doi httpdxdoiorgjeee\"]],\n        columns: [[\"number\", \"index\"], [\"string\", \"abstract\"], [\"string\", \"proc_abstract\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    ",
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>abstract</th>\n",
              "      <th>proc_abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>It is difficult to motivate learners to learn ...</td>\n",
              "      <td>difficult motivate learner learn abstract comp...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            abstract                                      proc_abstract\n",
              "0  It is difficult to motivate learners to learn ...  difficult motivate learner learn abstract comp..."
            ]
          },
          "metadata": {},
          "execution_count": 13
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 22.7 s (started: 2021-12-09 01:08:05 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pd6-M5SVgsep",
        "outputId": "b4a00287-1ac0-4e3a-b1e8-35f25225b6b5"
      },
      "source": [
        "#@title Normalización (texto): title\n",
        "n_feature = \"title\" \n",
        "n_eliminar_rows_con_feature_nulo = True #@param {type:\"boolean\"}\n",
        "n_reemplazar_feature_nulo_por_unknown = False #@param {type:\"boolean\"}\n",
        "n_eliminar_espacios_iniciales_finales = True #@param {type:\"boolean\"}\n",
        "n_pasar_minuscula = True #@param {type:\"boolean\"}\n",
        "n_eliminar_espacios = True #@param {type:\"boolean\"}\n",
        "n_eliminar_stopwords = True #@param {type:\"boolean\"}\n",
        "n_expandir_contracciones = True #@param {type:\"boolean\"}\n",
        "n_eliminar_caracteres_especiales = True #@param {type:\"boolean\"}\n",
        "n_correccion_ortografica = True #@param {type:\"boolean\"}\n",
        "n_reemplar_doble_espacios = True #@param {type:\"boolean\"}\n",
        "n_eliminar_acentos = False #@param {type:\"boolean\"}\n",
        "n_elimina_tag = False #@param {type:\"boolean\"}\n",
        "n_elimina_menciones = False #@param {type:\"boolean\"}\n",
        "n_elimina_black_word_list = True #@param {type:\"boolean\"}\n",
        "n_elimina_sino_esta_en_white_char_list = False #@param {type:\"boolean\"}\n",
        "n_lemmatize_text_spacy = True #@param {type:\"boolean\"}\n",
        "n_stemmer_text_spacy = False #@param {type:\"boolean\"}\n",
        "n_get_only_char_to_lower = True #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "df1 = normalizar_text(df1,n_feature, n_eliminar_rows_con_feature_nulo,\n",
        "                n_reemplazar_feature_nulo_por_unknown ,\n",
        "                n_eliminar_espacios_iniciales_finales ,\n",
        "                n_pasar_minuscula ,\n",
        "                n_eliminar_espacios ,\n",
        "                n_eliminar_stopwords,\n",
        "                n_expandir_contracciones,\n",
        "                n_eliminar_caracteres_especiales,\n",
        "                n_correccion_ortografica,\n",
        "                n_reemplar_doble_espacios,\n",
        "                n_eliminar_acentos,\n",
        "                n_elimina_tag,\n",
        "                n_elimina_menciones,\n",
        "                n_elimina_black_word_list,\n",
        "                n_elimina_sino_esta_en_white_char_list,\n",
        "                n_lemmatize_text_spacy,\n",
        "                n_stemmer_text_spacy,\n",
        "                n_get_only_char_to_lower,\n",
        "                True)\n",
        "\n",
        "df1[['title','proc_title']].head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=======================================\n",
            "* Normalización de feature text:  title  en  proc_title\n",
            "* Remover los rows con nulos, se removieron: 0\n",
            "* Eliminamos los espacios iniciales y finales\n",
            "* Pasa palabras a minúsculas\n",
            "* Removemos espacios en blanco extras\n",
            "* Removemos las stopwords\n",
            "* Expandimos las contracciones en ingles\n",
            "* Removemos los carácteres especiales\n",
            "* Realizamos corrección ortográfica rapida en ingles\n",
            "* Reemplazamos los doble espacios por uno solo\n",
            "* Lematizamos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faW0oOuB7Ny9"
      },
      "source": [
        "---\n",
        "## **<font color='blue'>Gráficos para una primera visualización de title y abstract</font>**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SF37U-ReWpTN"
      },
      "source": [
        "---\n",
        "### **<font color='blue'>Funciones</font>**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSNM9FGILXBc"
      },
      "source": [
        "#Ver optimización de estas funciones\n",
        "from wordcloud import WordCloud\n",
        "def wc_from_dict(word_count,title, file_name=\"\"):\n",
        "    wc = WordCloud(\n",
        "        #max_words=40,  # Número máximo de palabras\n",
        "        max_font_size = 200, # tamaño de fuente máximo\n",
        "        background_color=\"white\",  # Establecer el fondo en blanco, el valor predeterminado es negro\n",
        "        width = 1500,  # Establecer el ancho de la imagen\n",
        "        height= 960,  # Establecer la altura de la imagen\n",
        "        margin= 10  # Establecer el borde de la imagen\n",
        "    )\n",
        "    wc.generate_from_frequencies(word_count)  # Generar nube de palabras del diccionario\n",
        "    plt.figure(figsize = (15, 15), facecolor = None)\n",
        "    plt.title(title)\n",
        "    plt.imshow(wc)  # Mostrar nube de palabras\n",
        "    plt.axis('off')  # Cerrar el eje\n",
        "    plt.show()  # Mostrar imagen\n",
        "    if file_name!=\"\":\n",
        "      wc.to_file(file_name)  # Guardar imagen\n",
        "\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "def generate_dict(df,feature,val_reverse):\n",
        "    #creamos tag de todos lo feature\n",
        "    card_docs = [TaggedDocument(doc.split(' '), [i]) \n",
        "                for i, doc in enumerate(df[feature])]\n",
        "\n",
        "    #cargamos el diccionario y contamos frecuencia\n",
        "    diccionario_frecuencias = {}\n",
        "    #x = [\" \", 0]\n",
        "    for doc in card_docs:\n",
        "      for words in doc:\n",
        "        for word in words:\n",
        "          palabra = str(word)\n",
        "          if palabra in diccionario_frecuencias:\n",
        "              diccionario_frecuencias[palabra] += 1\n",
        "          else:\n",
        "              diccionario_frecuencias[palabra] = 1\n",
        "          \n",
        "    #ordenamos descendente      \n",
        "    from collections import OrderedDict\n",
        "    sortedDict = OrderedDict(sorted(diccionario_frecuencias.items(), key=lambda x: x[1],reverse=val_reverse))\n",
        "\n",
        "    return sortedDict\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "def bar_from_dict(dic_frec, top, title, file_name=\"\"):\n",
        "    the_dict = {}\n",
        "    count = 0\n",
        "    for dic in dic_frec:\n",
        "      the_dict[dic] = dic_frec[dic]\n",
        "      \n",
        "      count +=1\n",
        "      if count > top:\n",
        "        break\n",
        "\n",
        "    x = list(the_dict.keys())\n",
        "    y = list(the_dict.values())\n",
        "\n",
        "    fig = plt.figure(figsize = (30, 15), facecolor = None)\n",
        "    plt.rcParams.update({'font.size': 22})\n",
        "    plt.title(title)\n",
        "    plt.bar( x,y)\n",
        "    plt.xlabel('word')\n",
        "    plt.ylabel('count')\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.show()\n",
        "    if file_name!=\"\":\n",
        "      fig.savefig(file_name)  # Guardar imagen\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSEJtikTHsCR"
      },
      "source": [
        "---\n",
        "### **<font color='blue'>Titles</font>**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARgg0wO7XjjL"
      },
      "source": [
        "#creamos diccionario de palabras\n",
        "dic_frec = generate_dict(df1,\"title\",True)\n",
        "\n",
        "print('------------------------- Titles prenormalizados ----------------------------------------------------------')\n",
        "wc_from_dict(dic_frec,\"Gráfico nube de palabras title original\")\n",
        "\n",
        "print('-------------------------- Titles normalizados y stematizados ---------------------------------------------')\n",
        "dic_frec_proc = generate_dict(df1,\"proc_title\",True)\n",
        "wc_from_dict(dic_frec_proc,\"Gráfico nube de palabras title procesado\")\n",
        "\n",
        "\n",
        "bar_from_dict(dic_frec, 40, \"Gráfico frecuencia de palabras de title\")\n",
        "bar_from_dict(dic_frec_proc, 40, \"Gráfico frecuencia de palabras de title procesado\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1UQYfUqH6Ek"
      },
      "source": [
        "---\n",
        "### **<font color='blue'>Abstract</font>**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23I8Lag4Li9d"
      },
      "source": [
        "#creamos diccionario de palabras\n",
        "dic_frec = generate_dict(df1,\"abstract\",True)\n",
        "\n",
        "print('------------------------- Abstracts prenormalizados ----------------------------------------------------------')\n",
        "wc_from_dict(dic_frec,\"Gráfico nube de palabras abstract original\")\n",
        "\n",
        "print('-------------------------- Abstracts normalizados y stematizados ---------------------------------------------')\n",
        "dic_frec_proc = generate_dict(df1,\"proc_abstract\",True)\n",
        "wc_from_dict(dic_frec_proc,\"Gráfico nube de palabras abstract procesado\")\n",
        "\n",
        "bar_from_dict(dic_frec, 40, \"Gráfico frecuencia de la palabras abstract\")\n",
        "bar_from_dict(dic_frec_proc, 40, \"Gráfico frecuencia de la palabras abstract procesado\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHQO9zMM8lnE"
      },
      "source": [
        "---\n",
        "## **<font color='blue'>Borramos feature ya procesados</font>**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e-zY2Dh0x7_"
      },
      "source": [
        "#eliminamos los originales de las procesadas\n",
        "df1.drop('title', axis='columns', inplace=True)\n",
        "df1.drop('pubmed_id', axis='columns', inplace=True)\n",
        "df1.drop('abstract', axis='columns', inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNxloA7txf4S"
      },
      "source": [
        "---\n",
        "# **<font color='blue'>Preparacion del DataSet</font>**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PABP_gNfEVc"
      },
      "source": [
        "# Clusterizado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9d0CaUUfJA0"
      },
      "source": [
        "import string\n",
        "import collections\n",
        " \n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from pprint import pprint\n",
        " \n",
        " \n",
        "def process_text(text, stem=True):\n",
        "    \"\"\" Tokenize text and stem words removing punctuation \"\"\"\n",
        "    #text = text.translate(None, string.punctuation)\n",
        "    tokens = word_tokenize(text)\n",
        " \n",
        "    if stem:\n",
        "        stemmer = PorterStemmer()\n",
        "        tokens = [stemmer.stem(t) for t in tokens]\n",
        " \n",
        "    return tokens\n",
        " \n",
        " \n",
        "def cluster_texts(texts, clusters=3):\n",
        "    \"\"\" Transform texts to Tf-Idf coordinates and cluster texts using K-Means \"\"\"\n",
        "    vectorizer = TfidfVectorizer(tokenizer=process_text,\n",
        "                                 stop_words=stopwords.words('english'),\n",
        "                                 max_df=0.5,\n",
        "                                 min_df=0.1,\n",
        "                                 lowercase=True)\n",
        " \n",
        "    tfidf_model = vectorizer.fit_transform(texts)\n",
        "    km_model = KMeans(n_clusters=clusters, random_state=42)\n",
        "    km_model.fit(tfidf_model)\n",
        " \n",
        "    clustering = collections.defaultdict(list)\n",
        " \n",
        "    for idx, label in enumerate(km_model.labels_):\n",
        "        clustering[label].append(idx)\n",
        " \n",
        "    return clustering\n",
        " \n",
        " \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nYWddUJOe0X"
      },
      "source": [
        "## Unificación de feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmOC8Lf1tIrg"
      },
      "source": [
        "df1.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKz1lNmwwwyV"
      },
      "source": [
        "df1.replace({'include?': {'SI': 1, 'NO': 0}}, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kkQ4c6qAjEu"
      },
      "source": [
        "df1[\"text_to_proc\"] = df1[\"creator\"].map(str) + \" \" + df1[\"proc_title\"].map(str) + \" \" + df1['proc_abstract'].map(str)\\\n",
        " + \" \" + df1['subtype'].map(str)+\" \"+df1['affilname'].map(str)+\" \"+df1['publicationName'].map(str)+\" \"+df1['aggregationType'].map(str)+\" \"+df1['proc_pubmed_id'].map(str)\n",
        "\n",
        "#limpiamos y pasamos a caracter por ultima vez\n",
        "df1.loc[:,\"text_to_proc\",] = df1.loc[:,\"text_to_proc\",].apply(lambda x: Normalization.get_only_char_to_lower(str(x)))\n",
        "\n",
        "df1[[\"text_to_proc\", \"include?\"]]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ek2XibE6i2kp"
      },
      "source": [
        "articles = df1['text_to_proc'].tolist()\n",
        "numclust=2\n",
        "clusters = cluster_texts(articles, numclust)\n",
        "resu=dict(clusters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGfxagxKl-u2"
      },
      "source": [
        "df1['include?'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yc2snrHak00J"
      },
      "source": [
        "for i in range(0,numclust):\n",
        "  print(f\"Cluster {i}\")\n",
        "  print(df1.iloc[resu[i]]['include?'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8qdneeYOyW_"
      },
      "source": [
        "## Generar dataset desde el mejor claster\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Cxym2j6yJXF"
      },
      "source": [
        "clustSelected= 0 if (df1.iloc[resu[0]]['include?'].sum()>df1.iloc[resu[1]]['include?'].sum()) else 1\n",
        "dfCluster=df1.iloc[resu[clustSelected]]\n",
        "# Nos guardamos el cluster completo donde cayeron los positivos\n",
        "dfCluster=dfCluster[[\"text_to_proc\", \"include?\"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pYb35gsPFzv"
      },
      "source": [
        "## Guardado de los cvs procesados\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAHWUfYIPUnl"
      },
      "source": [
        "dfCluster.head"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6KAK3z5ZQwt",
        "outputId": "3abd38d1-8545-47c4-c982-c0a82c57e820"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(665, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 4.66 ms (started: 2021-12-09 00:58:34 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmpTjdYSPMaC",
        "outputId": "aed9d961-cdfd-4e30-90af-f5966a1b825d"
      },
      "source": [
        "df1.to_csv(\"dtnormlemati.csv\",index=False)\n",
        "#df1.to_csv(\"dtnormestemi.csv\",index=False)\n",
        "#df1.to_csv(\"dsRob.csv\",index=False)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 52.6 ms (started: 2021-12-09 01:12:09 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfCluster.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTQiVyUAZGvv",
        "outputId": "4779c0dd-de70-4132-bd68-44b4d54375df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(232, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 3.52 ms (started: 2021-12-09 01:11:03 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLNhzxLMPmm6",
        "outputId": "318f2bcf-fb30-4861-a099-a72e35c0503b"
      },
      "source": [
        "dfCluster.to_csv(\"dtrainlemati.csv\",index=False)\n",
        "#dfCluster.to_csv(\"dtrainestemi.csv\",index=False)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 18.2 ms (started: 2021-12-09 01:12:21 +00:00)\n"
          ]
        }
      ]
    }
  ]
}